# Spark Documentation Execution Plan (Linus-Approved)

## Overview
**Total: ~20 essential files** (down from 50)
**Timeline: 4 phases, iterative delivery**
**Strategy: Sequential phases, parallel agents within phases**

---

## PHASE 1: MAKE IT WORK (Week 1)
**Goal**: Users can install and run their first workflow in under 30 minutes

### Parallel Execution Group 1A (3 agents simultaneously)

#### Agent 1: Fix Root Documentation
**Files to fix/update (3 files):**
1. `introduction.mdx`
   - Remove YAML workflow bullshit
   - Add clear architecture diagram (mermaid)
   - Real value prop (not marketing wank)
   - Links to actual getting-started flow

2. `installation.mdx`
   - Fix command names (`spark` → `automagik-spark`)
   - Add Docker Compose setup (TESTED)
   - Prerequisites with version checks
   - Troubleshooting common install issues

3. `quickstart.mdx`
   - Real flow: source → sync → schedule → verify
   - Copy-paste commands that WORK
   - Expected output for each step
   - "What to do when it fails" section

**Validation**: Run through quickstart on clean system, time it

---

#### Agent 2: Architecture Overview
**File to create (1 file):**
1. `concepts/architecture-overview.mdx`
   - ONE page with THE diagram:
     ```
     ┌─────────────┐
     │  FastAPI    │ ← REST API (8883)
     │  (Python)   │
     └──────┬──────┘
            │
     ┌──────┴───────┬──────────┬────────────┐
     │              │          │            │
     ▼              ▼          ▼            ▼
     PostgreSQL   Redis   Celery Beat   Workers
     (state)     (queue)  (scheduler)  (execute)
                            │
                            ▼
                    ┌───────────────┐
                    │   Adapters    │
                    └───────┬───────┘
                            │
                    ┌───────┴────────┐
                    │                │
                    ▼                ▼
                LangFlow          Hive
     ```
   - Component responsibilities (1-2 sentences each)
   - Data flow explanation
   - "How a schedule becomes an execution"
   - Port/connection diagram

**Validation**: Someone unfamiliar can explain how Spark works

---

#### Agent 3: Common Errors
**File to create (1 file):**
1. `troubleshooting/common-errors.mdx`
   - Top 10 actual errors from:
     - GitHub issues
     - Source code error messages
     - Your own testing
   - Format per error:
     ```
     ### Error: "Connection refused to Redis"
     **What it means**: Worker can't connect to task queue
     **How to fix**:
     1. Check Redis is running: `redis-cli ping`
     2. Check CELERY_BROKER_URL in .env
     3. Check firewall/ports
     **Copy-paste solution**: [exact commands]
     ```
   - Common installation problems
   - Common runtime problems
   - When to check logs (and where they are)

**Validation**: Each error has been personally reproduced and solved

---

### Sequential After 1A: Review & Ship
**1 person reviews all Phase 1 docs**:
- Terminology consistency
- Cross-links work
- Commands actually run
- Timing the quickstart (<30 min)

**Ship Phase 1** before starting Phase 2

---

## PHASE 2: API & CLI TRUTH (Week 2)
**Goal**: Complete, accurate technical reference

### Parallel Execution Group 2A (3 agents simultaneously)

#### Agent 1: API Reference Complete
**Files to expand/create (6 files):**

1. `api/rest-overview.mdx` - EXPAND
   - Base URL and port
   - Authentication (X-API-Key header)
   - Response format (success/error structure)
   - Error codes table (with actual HTTP status codes)
   - No rate limiting (document reality)

2. `api/sources.mdx` - NEW
   - GET /api/v1/sources
   - POST /api/v1/sources
   - PATCH /api/v1/sources/{id}
   - DELETE /api/v1/sources/{id}
   - Each with: curl example, response body, error cases

3. `api/workflows.mdx` - EXPAND
   - GET /api/v1/workflows
   - GET /api/v1/workflows/remote
   - POST /api/v1/workflows/sync/{id}
   - POST /api/v1/workflows/{id}/run
   - DELETE /api/v1/workflows/{id}

4. `api/schedules.mdx` - EXPAND
   - All CRUD operations
   - POST /api/v1/schedules/{id}/enable
   - POST /api/v1/schedules/{id}/disable
   - Input data configuration

5. `api/tasks.mdx` - RENAME FROM executions.mdx + EXPAND
   - GET /api/v1/tasks (with filters)
   - GET /api/v1/tasks/{id}
   - POST /api/v1/tasks/{id}/retry
   - Log retrieval

6. `api/workers.mdx` - NEW
   - Worker status endpoint (if exists)
   - Health monitoring
   - Or state "no API endpoints, use CLI"

**Validation**: Test every endpoint with curl, paste actual responses

---

#### Agent 2: CLI Reference Complete
**Files to create (3 files):**

1. `cli/overview.mdx`
   - Installation: `pip install automagik-spark`
   - Command structure: `automagik-spark <group> <command>`
   - Global flags: `--help`, `--version`
   - Where config lives
   - Environment variables

2. `cli/commands.mdx` (single comprehensive file)
   - **Sources**: add, list, update, delete
   - **Workflows**: list, sync, run, delete
   - **Schedules**: create, list, update, delete, set-expression, set-input
   - **Tasks**: list, view, retry, create
   - **Workers**: start, stop, status, logs
   - **API**: start (with options)
   - **DB**: migrate, upgrade, downgrade

   Format per command:
   ```
   ### automagik-spark sources add

   Add a new workflow source.

   **Usage**:
   automagik-spark sources add --name NAME --type TYPE --url URL --api-key KEY

   **Options**:
   - `--name` (required): Source identifier
   - `--type` (required): langflow | automagik-hive
   - `--url` (required): Base URL of source
   - `--api-key` (required): API authentication key

   **Example**:
   automagik-spark sources add \
     --name my-langflow \
     --type langflow \
     --url http://localhost:7860 \
     --api-key sk-abc123

   **Output**:
   [actual output from running command]
   ```

3. `cli/examples.mdx`
   - Common workflows using CLI
   - "Add source and sync first workflow"
   - "Create daily schedule"
   - "Debug failed task"
   - "Scale workers"

**Validation**: Run every command, paste actual output

---

#### Agent 3: Configuration Reference
**File to create (1 mega file):**

1. `config/configuration-reference.mdx`

   **Sections**:

   **Basic Setup**:
   - All environment variables (alphabetical)
   - Required vs optional (explicit)
   - Default values
   - Example .env file

   **Database Configuration**:
   - PostgreSQL connection string format
   - Database creation commands
   - Migration commands
   - Backup strategy

   **Redis Configuration**:
   - Broker URL format
   - Result backend
   - Memory limits

   **Celery Configuration**:
   - Worker concurrency
   - Beat scheduler
   - Retry policies

   **Security**:
   - API key generation
   - Encryption key setup
   - CORS configuration
   - Production checklist

   **Logging**:
   - Log levels
   - Log locations
   - Rotation

**Validation**: Someone can go production using only this doc

---

### Sequential After 2A: Review & Ship
**Review for**:
- Every API endpoint tested
- Every CLI command tested
- No contradictions between API/CLI docs
- Config values match actual code defaults

**Ship Phase 2** before Phase 3

---

## PHASE 3: REAL EXAMPLES (Week 3)
**Goal**: Copy-paste workflows that actually work

### Parallel Execution Group 3A (4 agents simultaneously)

#### Agent 1: Simple Schedule Example
**File to create:**
1. `examples/simple-schedule.mdx`
   - Goal: Schedule a workflow to run every 5 minutes
   - Prerequisites listed
   - Step-by-step with EXACT commands
   - How to verify it's working
   - How to check logs
   - How to stop it
   - Common mistakes section

**Validation**: Fresh install to scheduled workflow in 10 minutes

---

#### Agent 2: LangFlow Integration Example
**File to create:**
1. `examples/langflow-integration.mdx`
   - Prerequisites: LangFlow installed
   - Create simple flow in LangFlow
   - Add LangFlow as source in Spark
   - Identify input/output components
   - Sync the flow
   - Create schedule
   - Verify execution
   - Check task results
   - Troubleshooting section

**Validation**: Tested with real LangFlow instance

---

#### Agent 3: Hive Integration Example
**File to create:**
1. `examples/hive-integration.mdx`
   - Prerequisites: Hive running
   - Create simple agent in Hive
   - Add Hive as source in Spark
   - Sync agent vs team vs workflow (differences)
   - Create schedule
   - Verify execution
   - Troubleshooting section

**Validation**: Tested with real Hive instance

---

#### Agent 4: Production Deployment
**File to create:**
1. `examples/production-deployment.mdx`
   - Docker Compose that WORKS:
     ```yaml
     version: '3.8'
     services:
       postgres:
         # exact config
       redis:
         # exact config
       spark-api:
         # exact config
       spark-worker:
         # exact config with scaling
       spark-beat:
         # exact config
     ```
   - Environment variable setup
   - Initial database setup
   - Health check commands
   - Monitoring setup (logs, metrics)
   - Backup strategy
   - Update procedure
   - Scaling workers

**Validation**: Deploy to fresh server, runs for 24 hours

---

### Sequential After 3A: Review & Ship
**Review for**:
- Every example tested on clean system
- Timing matches stated (<30 min for simple, etc.)
- Troubleshooting sections cover real issues encountered

**Ship Phase 3** before Phase 4

---

## PHASE 4: DEEP DIVES (Week 4)
**Goal**: Advanced usage for power users
**NOTE**: Only do this if Phases 1-3 are DONE and user feedback is positive

### Parallel Execution Group 4A (3 agents simultaneously)

#### Agent 1: Essential Concepts
**Files to create (3 files):**

1. `concepts/scheduling-internals.mdx`
   - How Celery Beat monitors schedules
   - Cron expression parsing (croniter)
   - Interval vs cron vs one-time
   - Timezone handling
   - Task creation from schedules
   - What happens when schedule fires

2. `concepts/task-execution.mdx`
   - Task lifecycle states
   - Worker picks task from queue
   - Adapter invocation
   - Input/output handling
   - Retry logic (with backoff)
   - Error capture
   - Result storage

3. `concepts/adapter-system.mdx`
   - What adapters do
   - LangFlow adapter (component-based I/O)
   - Hive adapter (different APIs for agent/team/workflow)
   - Common adapter interface
   - When to create custom adapter

**Validation**: Technical accuracy verified against source code

---

#### Agent 2: Advanced Topics
**Files to create (2 files):**

1. `advanced/custom-adapters.mdx`
   - Adapter base class
   - Required methods
   - Registration process
   - Input transformation
   - Output normalization
   - Error handling patterns
   - Testing strategy
   - Real example: custom REST API adapter

2. `advanced/scaling-production.mdx`
   - Worker scaling (horizontal)
   - Database connection pooling
   - Redis memory management
   - Queue priority strategies
   - Long-running task handling
   - Monitoring and alerting
   - Performance metrics

**Validation**: At least one custom adapter built and tested

---

#### Agent 3: Integration Guides
**Files to create (2 files):**

1. `integrations/langflow.mdx`
   - Deep dive on LangFlow integration
   - Component identification details
   - Input/output mapping
   - Common LangFlow patterns
   - Debugging sync issues
   - Version compatibility

2. `integrations/hive.mdx`
   - Deep dive on Hive integration
   - Agent vs Team vs Workflow differences
   - API endpoint variations
   - Authentication details
   - Debugging sync issues

**Validation**: Covers edge cases found during testing

---

### Sequential After 4A: Review & Ship
**Review for**:
- Technical depth appropriate for advanced users
- No marketing fluff
- Code examples work
- Links to source code where relevant

**Ship Phase 4**

---

## DOCS.JSON NAVIGATION UPDATE

After each phase ships, update navigation:

```json
{
  "tab": "Spark",
  "groups": [
    {
      "group": "Getting Started",
      "pages": [
        "spark/introduction",
        "spark/installation",
        "spark/quickstart",
        "spark/concepts/architecture-overview"
      ]
    },
    {
      "group": "Examples",
      "pages": [
        "spark/examples/simple-schedule",
        "spark/examples/langflow-integration",
        "spark/examples/hive-integration",
        "spark/examples/production-deployment"
      ]
    },
    {
      "group": "API Reference",
      "pages": [
        "spark/api/rest-overview",
        "spark/api/sources",
        "spark/api/workflows",
        "spark/api/schedules",
        "spark/api/tasks",
        "spark/api/workers"
      ]
    },
    {
      "group": "CLI Reference",
      "pages": [
        "spark/cli/overview",
        "spark/cli/commands",
        "spark/cli/examples"
      ]
    },
    {
      "group": "Configuration",
      "pages": [
        "spark/config/configuration-reference"
      ]
    },
    {
      "group": "Advanced",
      "pages": [
        "spark/concepts/scheduling-internals",
        "spark/concepts/task-execution",
        "spark/concepts/adapter-system",
        "spark/advanced/custom-adapters",
        "spark/advanced/scaling-production",
        "spark/integrations/langflow",
        "spark/integrations/hive"
      ]
    },
    {
      "group": "Troubleshooting",
      "pages": [
        "spark/troubleshooting/common-errors"
      ]
    }
  ]
}
```

---

## EXECUTION RULES

### Before Writing ANY Doc:
1. ✅ Test the feature/command yourself
2. ✅ Capture actual output
3. ✅ Note actual errors encountered
4. ✅ Time how long it takes

### While Writing:
1. ✅ Use consistent terminology (workflow, task, schedule, source)
2. ✅ Link to related docs
3. ✅ Include troubleshooting for common issues
4. ✅ Copy-paste commands that work

### After Writing:
1. ✅ Have someone else test it
2. ✅ Fix issues found
3. ✅ Update if anything changed

### Never:
1. ❌ Document features that don't exist
2. ❌ Use marketing language
3. ❌ Make up example output
4. ❌ Skip testing to save time

---

## PARALLEL AGENT PROTOCOL

When running agents in parallel:

**BEFORE starting:**
1. All agents use the SAME terminology doc
2. All agents test on the SAME Spark version
3. All agents know which other docs exist (avoid duplication)

**DURING execution:**
1. Each agent works independently
2. No blocking on other agents
3. Use placeholders for cross-links (fix in review)

**AFTER completion:**
1. ONE reviewer checks all docs together
2. Fix terminology inconsistencies
3. Add proper cross-links
4. Verify no duplicate content

---

## SUCCESS METRICS (For Real)

### Phase 1 Success:
- [ ] New user → first scheduled workflow in <30 minutes
- [ ] Top 3 install errors documented with solutions
- [ ] Architecture diagram makes sense to non-experts

### Phase 2 Success:
- [ ] Every API endpoint tested and documented
- [ ] Every CLI command tested and documented
- [ ] Someone can deploy to production using config doc

### Phase 3 Success:
- [ ] 4 examples all tested on clean systems
- [ ] Each example includes "what could go wrong"
- [ ] Docker Compose runs for 24 hours without issues

### Phase 4 Success:
- [ ] Advanced users can build custom adapters
- [ ] Scaling guidance based on real load testing
- [ ] Integration docs cover edge cases

---

## TIMELINE

- **Week 1**: Phase 1 (foundation) - 5 files
- **Week 2**: Phase 2 (reference) - 10 files
- **Week 3**: Phase 3 (examples) - 4 files
- **Week 4**: Phase 4 (advanced) - 9 files

**Total: ~28 files** (not 50)

**Delivery**: After each phase, not at the end

---

## READY TO EXECUTE

This plan is:
- ✅ Practical (achievable in timeline)
- ✅ Tested (every doc requires testing)
- ✅ Iterative (ship after each phase)
- ✅ Parallelizable (where it makes sense)
- ✅ User-focused (not marketing-focused)

Let's ship Phase 1.
