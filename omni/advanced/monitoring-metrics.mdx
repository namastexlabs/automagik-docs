---
title: "Monitoring & Metrics"
description: "Production monitoring, observability, and alerting for Automagik OMNI"
icon: "chart-line"
---

## Key Metrics to Track

### Application Metrics

**Message Processing**:
- **Messages per minute**: Throughput indicator
- **Average response time**: End-to-end latency
- **Agent response time**: AI processing time
- **Success rate**: Percentage of completed messages
- **Error rate**: Failed messages per minute

**Instance Metrics**:
- **Active instances**: Number of running instances
- **Connected instances**: WhatsApp/Discord connection status
- **Instance health**: Per-instance status checks

**Resource Metrics**:
- **CPU usage**: Per-process and system-wide
- **Memory usage**: Application and system memory
- **Database connections**: Active and idle connections
- **API latency**: Request/response times

**Business Metrics**:
- **Active users**: Unique users per day/week/month
- **Conversation length**: Average messages per session
- **Peak hours**: Time-based usage patterns
- **Popular features**: Most-used message types

---

## Built-in Trace System

### Using Trace Analytics

**Real-time Metrics**:

```bash
# Get analytics summary
curl -H "x-api-key: YOUR_KEY" \
  http://localhost:8882/api/v1/traces/analytics/summary | jq

# Response:
{
  "total_messages": 15432,
  "completed": 14987,
  "failed": 445,
  "success_rate": 97.1,
  "avg_processing_time_ms": 1234,
  "avg_agent_response_time_ms": 1150,
  "time_range": {
    "start": "2025-11-01T00:00:00Z",
    "end": "2025-11-04T23:59:59Z"
  },
  "by_instance": {
    "sales-bot": {
      "total": 8432,
      "completed": 8201,
      "failed": 231,
      "avg_response_time_ms": 1100
    },
    "support-bot": {
      "total": 7000,
      "completed": 6786,
      "failed": 214,
      "avg_response_time_ms": 1350
    }
  }
}
```

**Query Traces by Filters**:

```bash
# Failed messages in last hour
START_TIME=$(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ)
curl -H "x-api-key: YOUR_KEY" \
  "http://localhost:8882/api/v1/traces?status=failed&start_date=$START_TIME" | jq

# Slow messages (> 5 seconds)
curl -H "x-api-key: YOUR_KEY" \
  "http://localhost:8882/api/v1/traces?limit=100" | \
  jq '.[] | select(.processing_time_ms > 5000) | {phone, processing_time_ms, created_at}'

# Messages by specific user
curl -H "x-api-key: YOUR_KEY" \
  "http://localhost:8882/api/v1/traces?phone=%2B1234567890&limit=50" | jq
```

**Custom Analytics Queries**:

```python
# analytics.py - Custom trace analysis
import requests
from datetime import datetime, timedelta

OMNI_URL = "http://localhost:8882"
API_KEY = "your-api-key"

def get_hourly_metrics(hours=24):
    """Get hourly message volume"""
    traces = requests.get(
        f"{OMNI_URL}/api/v1/traces?limit=10000",
        headers={"x-api-key": API_KEY}
    ).json()

    hourly = {}
    for trace in traces:
        hour = trace["created_at"][:13]  # YYYY-MM-DDTHH
        if hour not in hourly:
            hourly[hour] = {"total": 0, "completed": 0, "failed": 0}
        hourly[hour]["total"] += 1
        hourly[hour][trace["status"]] += 1

    return hourly

def get_top_users(limit=10):
    """Get most active users"""
    traces = requests.get(
        f"{OMNI_URL}/api/v1/traces?limit=10000",
        headers={"x-api-key": API_KEY}
    ).json()

    users = {}
    for trace in traces:
        phone = trace["phone"]
        users[phone] = users.get(phone, 0) + 1

    return sorted(users.items(), key=lambda x: x[1], reverse=True)[:limit]

# Run analytics
print("Hourly Metrics:", get_hourly_metrics())
print("Top 10 Users:", get_top_users())
```

---

## Prometheus Integration

### Exposing Metrics Endpoint

**FastAPI Prometheus Exporter** (future feature):

```python
# metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import FastAPI, Response
import time

app = FastAPI()

# Define metrics
messages_total = Counter(
    'omni_messages_total',
    'Total messages processed',
    ['instance', 'status']
)

processing_time = Histogram(
    'omni_processing_seconds',
    'Message processing time',
    ['instance'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

agent_response_time = Histogram(
    'omni_agent_response_seconds',
    'Agent response time',
    ['instance'],
    buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

active_instances = Gauge(
    'omni_active_instances',
    'Number of active instances'
)

connected_instances = Gauge(
    'omni_connected_instances',
    'Number of connected instances',
    ['channel_type']
)

@app.middleware("http")
async def track_metrics(request, call_next):
    """Track request metrics"""
    start = time.time()
    response = await call_next(request)
    duration = time.time() - start

    if request.url.path.startswith("/api/v1/instances"):
        processing_time.labels(instance="api").observe(duration)

    return response

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )

# Track message processing
def track_message(instance: str, status: str, duration: float, agent_time: float):
    messages_total.labels(instance=instance, status=status).inc()
    processing_time.labels(instance=instance).observe(duration)
    agent_response_time.labels(instance=instance).observe(agent_time)
```

**Prometheus Configuration**:

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'omni'
    static_configs:
      - targets: ['localhost:8882']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']  # Node exporter

  - job_name: 'postgres'
    static_configs:
      - targets: ['localhost:9187']  # PostgreSQL exporter
```

**Run Prometheus**:

```bash
# Install Prometheus
wget https://github.com/prometheus/prometheus/releases/download/v2.47.0/prometheus-2.47.0.linux-amd64.tar.gz
tar -xzf prometheus-2.47.0.linux-amd64.tar.gz
cd prometheus-2.47.0.linux-amd64

# Start Prometheus
./prometheus --config.file=prometheus.yml

# Access: http://localhost:9090
```

---

## Grafana Dashboards

### Installation

```bash
# Install Grafana
sudo apt-get install -y software-properties-common
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -
sudo apt-get update
sudo apt-get install grafana

# Start Grafana
sudo systemctl enable grafana-server
sudo systemctl start grafana-server

# Access: http://localhost:3000
# Default login: admin/admin
```

**Add Prometheus Data Source**:

1. Go to Configuration â†’ Data Sources
2. Click "Add data source"
3. Select "Prometheus"
4. URL: `http://localhost:9090`
5. Click "Save & Test"

---

### OMNI Dashboard Example

**Dashboard JSON** (import into Grafana):

```json
{
  "dashboard": {
    "title": "Automagik OMNI Monitoring",
    "panels": [
      {
        "title": "Messages per Minute",
        "targets": [
          {
            "expr": "rate(omni_messages_total[1m])",
            "legendFormat": "{{instance}} - {{status}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Average Response Time",
        "targets": [
          {
            "expr": "rate(omni_processing_seconds_sum[5m]) / rate(omni_processing_seconds_count[5m])",
            "legendFormat": "{{instance}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Success Rate",
        "targets": [
          {
            "expr": "sum(rate(omni_messages_total{status=\"completed\"}[5m])) / sum(rate(omni_messages_total[5m])) * 100",
            "legendFormat": "Success %"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Active Instances",
        "targets": [
          {
            "expr": "omni_active_instances",
            "legendFormat": "Active"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Top 10 Slowest Messages",
        "targets": [
          {
            "expr": "topk(10, omni_processing_seconds)",
            "legendFormat": "{{instance}}"
          }
        ],
        "type": "table"
      }
    ]
  }
}
```

**Key Panels**:

1. **Message Throughput**:
   - Messages per minute (total, by instance, by status)
   - Line graph showing trends

2. **Response Times**:
   - Average, p50, p95, p99 response times
   - Agent response time distribution

3. **Success Rate**:
   - Overall success rate percentage
   - Failed message rate

4. **Resource Usage**:
   - CPU usage per process
   - Memory usage
   - Database connection pool

5. **Instance Health**:
   - Active vs total instances
   - Connection status by channel type

---

## Log Aggregation (ELK Stack)

### Elasticsearch + Logstash + Kibana

**Installation (Docker Compose)**:

```yaml
# docker-compose-elk.yml
version: '3.8'
services:
  elasticsearch:
    image: elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - es-data:/usr/share/elasticsearch/data

  logstash:
    image: logstash:8.10.0
    ports:
      - "5044:5044"
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: kibana:8.10.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

volumes:
  es-data:
```

**Logstash Configuration**:

```ruby
# logstash.conf
input {
  file {
    path => "/var/log/omni/*.log"
    start_position => "beginning"
    codec => json
  }
}

filter {
  if [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
    }
  }

  if [message] =~ /webhook/ {
    mutate {
      add_tag => ["webhook"]
    }
  }

  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "omni-logs-%{+YYYY.MM.dd}"
  }
}
```

**Start ELK Stack**:

```bash
docker-compose -f docker-compose-elk.yml up -d

# Access Kibana: http://localhost:5601
```

**Kibana Queries**:

```
# All errors in last hour
level:ERROR AND @timestamp:[now-1h TO now]

# Webhook failures
tags:webhook AND level:ERROR

# Slow messages (> 5 seconds)
processing_time_ms:>5000

# Messages from specific instance
instance_name:"sales-bot"

# Failed agent requests
agent_error:* OR agent_timeout:*
```

---

## APM Tools

### New Relic Integration

**Install New Relic Agent**:

```python
# Install New Relic
pip install newrelic

# Configure (newrelic.ini)
[newrelic]
license_key = YOUR_LICENSE_KEY
app_name = Automagik OMNI
monitor_mode = true
log_level = info

[newrelic:application]
developer_mode = false
```

**Run with New Relic**:

```bash
# Start OMNI with New Relic
NEW_RELIC_CONFIG_FILE=newrelic.ini newrelic-admin run-program \
  uvicorn src.api.app:app --host 0.0.0.0 --port 8882
```

**Custom Instrumentation**:

```python
# custom_metrics.py
import newrelic.agent

@newrelic.agent.background_task()
def process_message(message_data):
    """Track as background task"""
    # Process message
    pass

@newrelic.agent.function_trace()
def call_agent_api(message):
    """Trace function execution"""
    # Call agent
    pass

# Record custom metrics
newrelic.agent.record_custom_metric('Custom/Messages/Success', 1)
newrelic.agent.record_custom_metric('Custom/Processing/Time', duration_ms)
```

---

### DataDog Integration

**Install DataDog Agent**:

```bash
# Install DataDog
DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=YOUR_API_KEY bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)"

# Configure Python integration
pip install ddtrace

# Run OMNI with DataDog
ddtrace-run uvicorn src.api.app:app --host 0.0.0.0 --port 8882
```

**Custom Metrics**:

```python
# datadog_metrics.py
from datadog import initialize, statsd

initialize(
    api_key='YOUR_API_KEY',
    app_key='YOUR_APP_KEY'
)

# Track metrics
statsd.increment('omni.messages.received', tags=['instance:sales-bot'])
statsd.histogram('omni.processing.time', duration_ms)
statsd.gauge('omni.instances.active', active_count)
```

---

## Custom Metrics Collection

### Simple Metrics Server

```python
# simple_metrics.py
from fastapi import FastAPI
from collections import defaultdict
from datetime import datetime
import json

app = FastAPI()

# In-memory metrics storage
metrics = {
    "messages": defaultdict(int),
    "errors": defaultdict(int),
    "response_times": [],
    "instances": set()
}

def record_message(instance: str, status: str, duration: float):
    """Record message processing"""
    key = f"{instance}:{status}"
    metrics["messages"][key] += 1
    metrics["response_times"].append({
        "instance": instance,
        "duration": duration,
        "timestamp": datetime.utcnow().isoformat()
    })
    metrics["instances"].add(instance)

    # Keep only last 1000 response times
    if len(metrics["response_times"]) > 1000:
        metrics["response_times"] = metrics["response_times"][-1000:]

@app.get("/metrics/summary")
async def get_metrics():
    """Get metrics summary"""
    total_messages = sum(metrics["messages"].values())
    avg_response_time = sum(m["duration"] for m in metrics["response_times"]) / len(metrics["response_times"]) if metrics["response_times"] else 0

    return {
        "total_messages": total_messages,
        "active_instances": len(metrics["instances"]),
        "avg_response_time_ms": avg_response_time,
        "by_instance": dict(metrics["messages"]),
        "recent_messages": metrics["response_times"][-10:]
    }

@app.post("/metrics/reset")
async def reset_metrics():
    """Reset all metrics"""
    metrics["messages"].clear()
    metrics["errors"].clear()
    metrics["response_times"].clear()
    metrics["instances"].clear()
    return {"status": "reset"}

# Run: uvicorn simple_metrics:app --port 9000
```

---

## Alerting Strategies

### Alert Rules

**Critical Alerts** (immediate action required):

1. **Service Down**:
   - Condition: Health check fails for 2+ minutes
   - Action: Page on-call engineer
   - Priority: P0

2. **High Error Rate**:
   - Condition: Error rate > 5% for 5 minutes
   - Action: Send alert to team channel
   - Priority: P1

3. **Database Connection Failures**:
   - Condition: DB connection errors > 10 in 1 minute
   - Action: Page database admin
   - Priority: P0

**Warning Alerts** (investigate soon):

1. **Slow Response Times**:
   - Condition: p95 response time > 5 seconds for 10 minutes
   - Action: Email team
   - Priority: P2

2. **High Memory Usage**:
   - Condition: Memory > 90% for 5 minutes
   - Action: Slack notification
   - Priority: P2

3. **Increased Failed Messages**:
   - Condition: Failed messages > 100 in 1 hour
   - Action: Email team lead
   - Priority: P2

---

### Prometheus Alertmanager

**Alert Rules** (`alerts.yml`):

```yaml
groups:
  - name: omni_alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: sum(rate(omni_messages_total{status="failed"}[5m])) / sum(rate(omni_messages_total[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} over last 5 minutes"

      - alert: SlowResponseTime
        expr: histogram_quantile(0.95, omni_processing_seconds) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Slow response times"
          description: "P95 response time is {{ $value }}s"

      - alert: ServiceDown
        expr: up{job="omni"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "OMNI service is down"
          description: "Service has been down for 2 minutes"

      - alert: HighMemoryUsage
        expr: (node_memory_Active_bytes / node_memory_MemTotal_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"
```

**Alertmanager Configuration** (`alertmanager.yml`):

```yaml
global:
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

route:
  group_by: ['alertname', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'team-notifications'
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true
    - match:
        severity: warning
      receiver: 'slack'

receivers:
  - name: 'team-notifications'
    email_configs:
      - to: 'team@example.com'

  - name: 'slack'
    slack_configs:
      - channel: '#omni-alerts'
        title: 'OMNI Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
```

---

## Health Checks and Readiness Probes

### Application Health Endpoints

**Enhanced Health Check**:

```python
# health.py
from fastapi import APIRouter, Response, status
from sqlalchemy import text
import redis
import httpx

router = APIRouter()

@router.get("/health")
async def health_check():
    """Basic health check"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

@router.get("/health/detailed")
async def detailed_health():
    """Detailed health check with dependencies"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }

    # Check database
    try:
        db.execute(text("SELECT 1"))
        health_status["checks"]["database"] = "healthy"
    except Exception as e:
        health_status["checks"]["database"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"

    # Check Redis (if used)
    try:
        redis_client.ping()
        health_status["checks"]["redis"] = "healthy"
    except Exception as e:
        health_status["checks"]["redis"] = f"unhealthy: {str(e)}"

    # Check Evolution API
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get("http://localhost:18082", timeout=5)
            if resp.status_code == 200:
                health_status["checks"]["evolution_api"] = "healthy"
            else:
                health_status["checks"]["evolution_api"] = f"unhealthy: {resp.status_code}"
    except Exception as e:
        health_status["checks"]["evolution_api"] = f"unhealthy: {str(e)}"

    status_code = 200 if health_status["status"] == "healthy" else 503
    return Response(
        content=json.dumps(health_status),
        status_code=status_code,
        media_type="application/json"
    )

@router.get("/readiness")
async def readiness_check():
    """Kubernetes readiness probe"""
    # Check if app is ready to receive traffic
    if not db_initialized:
        return Response(status_code=503, content="Database not initialized")

    return {"status": "ready"}

@router.get("/liveness")
async def liveness_check():
    """Kubernetes liveness probe"""
    # Simple check that process is alive
    return {"status": "alive"}
```

**Kubernetes Probes**:

```yaml
# kubernetes-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: omni-api
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: omni-api
          image: automagik-omni:latest
          ports:
            - containerPort: 8882
          livenessProbe:
            httpGet:
              path: /liveness
              port: 8882
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /readiness
              port: 8882
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
```

---

## Distributed Tracing

### OpenTelemetry Integration (Future)

```python
# opentelemetry_config.py
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Configure tracer
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Configure Jaeger exporter
jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Instrument FastAPI
FastAPIInstrumentor.instrument_app(app)

# Custom spans
@tracer.start_as_current_span("process_message")
def process_message(message_data):
    with tracer.start_as_current_span("call_agent"):
        response = call_agent(message_data)

    with tracer.start_as_current_span("send_response"):
        send_message(response)

    return response
```

---

## Monitoring Checklist

**Essential Monitoring**:
- [ ] Application health checks
- [ ] Message throughput metrics
- [ ] Error rate tracking
- [ ] Response time monitoring
- [ ] Database performance
- [ ] Resource usage (CPU, memory)

**Advanced Monitoring**:
- [ ] Prometheus + Grafana dashboards
- [ ] Log aggregation (ELK or similar)
- [ ] APM tool integration
- [ ] Distributed tracing
- [ ] Custom business metrics

**Alerting**:
- [ ] Critical alerts configured
- [ ] Warning alerts configured
- [ ] Alert routing (email, Slack, PagerDuty)
- [ ] On-call schedule
- [ ] Runbooks for common alerts

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Scaling Production" icon="server" href="/omni/advanced/scaling-production">
    Infrastructure and scaling strategies
  </Card>
  <Card title="Troubleshooting" icon="wrench" href="/omni/troubleshooting/common-issues">
    Debug production issues
  </Card>
  <Card title="Custom Agents" icon="robot" href="/omni/advanced/custom-agents">
    Build optimized AI agents
  </Card>
  <Card title="API Reference" icon="code" href="/omni/api/instances">
    Complete API documentation
  </Card>
</CardGroup>
