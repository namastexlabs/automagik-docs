---
title: "Quick Start"
description: "Schedule your first workflow in under 30 minutes"
icon: "rocket"
---

## Schedule Your First Workflow in Under 30 Minutes

This guide walks you through the complete Spark workflow: connecting a source, syncing a workflow, creating a schedule, and verifying execution.

<Info>
**Prerequisites**: Complete the [installation](/spark/installation) first. You need Spark running with PostgreSQL and Redis.
</Info>

---

## The Real Workflow

Spark follows a four-step process:

1. **Add a workflow source** (LangFlow or Hive instance)
2. **Sync workflows** from that source
3. **Create a schedule** for a workflow
4. **Verify execution** and check results

Let's walk through each step.

---

## Step 1: Add a Workflow Source

First, connect Spark to a LangFlow or Hive instance where your workflows live.

### If you have LangFlow running:

```bash
# Add LangFlow as a source
automagik-spark sources add \
  --name "my-langflow" \
  --type "langflow" \
  --url "http://localhost:7860" \
  --api-key "your-langflow-api-key"
```

**Expected output:**
```
Health check passed: status ok
Version check passed: 1.0.65
Successfully added source: http://localhost:7860
```

### If you have Automagik Hive running:

```bash
# Add Hive as a source
automagik-spark sources add \
  --name "my-hive" \
  --type "automagik-agents" \
  --url "http://localhost:8000" \
  --api-key "your-hive-api-key"
```

### Verify the source was added:

```bash
automagik-spark sources list
```

**Expected output:**
```
┌──────────────────────────────────────┬─────────────────────┬──────────┬────────┬─────────┐
│ ID                                   │ URL                 │ Type     │ Status │ Version │
├──────────────────────────────────────┼─────────────────────┼──────────┼────────┼─────────┤
│ abc123-def456-ghi789                 │ http://localhost... │ langflow │ active │ 1.0.65  │
└──────────────────────────────────────┴─────────────────────┴──────────┴────────┴─────────┘
```

<Tip>
**Don't have LangFlow or Hive?** Install LangFlow with: `pip install langflow && langflow run`
</Tip>

---

## Step 2: Sync Workflows

Discover all available workflows from your sources:

```bash
# List all available workflows from all sources
automagik-spark workflows sync
```

**Expected output:**
```
┌────────────────────┬─────────────────────┬──────────────────────┬──────────┐
│ ID                 │ Name                │ Description          │ Source   │
├────────────────────┼─────────────────────┼──────────────────────┼──────────┤
│ flow-abc-123       │ daily-report        │ Generate daily stats │ langflow │
│ flow-def-456       │ data-processor      │ Process CSV data     │ langflow │
└────────────────────┴─────────────────────┴──────────────────────┴──────────┘

Command: sync <flow_id> • Sources: my-langflow
```

### Sync a specific workflow:

```bash
# Sync the workflow you want to schedule
automagik-spark workflows sync flow-abc-123
```

**Expected output:**
```
Successfully synced flow flow-abc-123
```

### Verify synced workflows:

```bash
automagik-spark workflows list
```

**Expected output:**
```
┌────────────┬──────────────┬────────────┬─────────────┬───────────┬──────────┬──────┬──────────────┐
│ ID         │ Name         │ Latest Run │ Tasks       │ Schedules │ Instance │ Type │ Last Updated │
├────────────┼──────────────┼────────────┼─────────────┼───────────┼──────────┼──────┼──────────────┤
│ workflow-1 │ daily-report │ NEW        │ 0 (0)       │ 0         │ langflow │ ...  │ 2 mins ago   │
└────────────┴──────────────┴────────────┴─────────────┴───────────┴──────────┴──────┴──────────────┘
```

---

## Step 3: Create a Schedule

Now schedule the synced workflow to run automatically.

### Interactive schedule creation:

```bash
automagik-spark schedules create
```

This starts an interactive wizard:

```
Available Workflows:
0: daily-report (0 schedules)

Select a workflow: 0

Schedule Type:
  0: Interval (e.g., every 30 minutes)
  1: Cron (e.g., every day at 8 AM)
  2: One-time (run once at a specific time)

Select schedule type: 1

Cron Examples:
  * * * * *     - Every minute
  */5 * * * *   - Every 5 minutes
  0 * * * *     - Every hour
  0 0 * * *     - Every day at midnight
  0 8 * * *     - Every day at 8 AM
  0 8 * * 1-5   - Every weekday at 8 AM

Enter cron expression: 0 9 * * *

Enter input value: {"message": "Generate daily report"}

Schedule created successfully with ID: schedule-abc-123
```

### Common schedule patterns:

<Tabs>
  <Tab title="Every 5 minutes">
    **Interval**: `5m`

    ```bash
    # When prompted for schedule type, select "0" (Interval)
    # Then enter: 5m
    ```
  </Tab>

  <Tab title="Every day at 8 AM">
    **Cron**: `0 8 * * *`

    ```bash
    # When prompted for schedule type, select "1" (Cron)
    # Then enter: 0 8 * * *
    ```
  </Tab>

  <Tab title="Every weekday at 9 AM">
    **Cron**: `0 9 * * 1-5`

    ```bash
    # When prompted for schedule type, select "1" (Cron)
    # Then enter: 0 9 * * 1-5
    ```
  </Tab>

  <Tab title="Run once now">
    **One-time**: `now`

    ```bash
    # When prompted for schedule type, select "2" (One-time)
    # When prompted for option, select "1" (Run now)
    ```
  </Tab>
</Tabs>

### Verify the schedule:

```bash
automagik-spark schedules list
```

**Expected output:**
```
┌──────────────┬──────────────┬──────────┬────────────┬────────────┬─────────────┬──────────┬────────┐
│ ID           │ Workflow     │ Type     │ Expression │ Next Run   │ Tasks       │ Input    │ Status │
├──────────────┼──────────────┼──────────┼────────────┼────────────┼─────────────┼──────────┼────────┤
│ schedule-123 │ daily-report │ cron     │ 0 9 * * *  │ Tomorrow   │ 0 (0)       │ {...}    │ ACTIVE │
│              │              │          │            │ 09:00 AM   │             │          │        │
└──────────────┴──────────────┴──────────┴────────────┴────────────┴─────────────┴──────────┴────────┘
```

<Info>
**Schedule Status Icons**:
- ● ACTIVE: Schedule is running
- ⏸ PAUSED: Schedule is paused (won't execute)
- ■ STOPPED: Schedule is stopped permanently
</Info>

---

## Step 4: Verify Execution

### Test run immediately:

Don't wait for the schedule—run it now to test:

```bash
automagik-spark workflows run workflow-123 --input "test message"
```

**Expected output:**
```
Task abc-def-123 completed successfully
Input: test message
Output: {"result": "Report generated successfully"}
```

### Check task history:

```bash
automagik-spark tasks list
```

**Expected output:**
```
┌──────────────┬──────────────┬─────────────┬────────┬────────────┬───────────────┐
│ ID           │ Workflow     │ Schedule    │ Status │ Created    │ Duration      │
├──────────────┼──────────────┼─────────────┼────────┼────────────┼───────────────┤
│ task-abc-123 │ daily-report │ schedule-12 │ ✓ OK   │ 2 mins ago │ 1.2s          │
└──────────────┴──────────────┴─────────────┴────────┴────────────┴───────────────┘
```

### View detailed task output:

```bash
automagik-spark tasks view task-abc-123
```

---

## What to Do When It Fails

<AccordionGroup>
  <Accordion title="Schedule created but tasks never execute">
    **Problem**: Workers may not be running.

    **Solution**:
    ```bash
    # Check worker status
    automagik-spark worker status

    # Start workers if not running
    automagik-spark worker start

    # Check worker logs
    automagik-spark worker logs
    ```
  </Accordion>

  <Accordion title="Task shows 'FAILED' status">
    **Problem**: Workflow execution failed.

    **Solution**:
    ```bash
    # View task details to see error
    automagik-spark tasks view task-abc-123

    # Common issues:
    # - Invalid input data format
    # - Source unreachable (check LangFlow/Hive is running)
    # - Workflow deleted from source
    ```
  </Accordion>

  <Accordion title="Source shows 'inactive' status">
    **Problem**: Spark can't connect to the workflow source.

    **Solution**:
    ```bash
    # Check if LangFlow/Hive is running
    curl http://localhost:7860/health  # for LangFlow
    curl http://localhost:8000/health  # for Hive

    # Update source with correct URL/API key
    automagik-spark sources update \
      --url "http://localhost:7860" \
      --api-key "new-api-key"
    ```
  </Accordion>

  <Accordion title="'No workflows found' after sync">
    **Problem**: The source has no workflows or sync failed.

    **Solution**:
    ```bash
    # Check if source has workflows
    automagik-spark workflows sync  # Lists remote workflows

    # Create a simple workflow in LangFlow/Hive first
    # Then sync again
    automagik-spark workflows sync flow-id
    ```
  </Accordion>
</AccordionGroup>

---

## Complete Example: End-to-End

Here's a complete example you can copy-paste (assuming LangFlow is running):

```bash
# 1. Add LangFlow source
automagik-spark sources add \
  --name "local-langflow" \
  --type "langflow" \
  --url "http://localhost:7860" \
  --api-key "sk-your-api-key"

# 2. List available workflows
automagik-spark workflows sync

# 3. Sync a specific workflow (replace with your flow ID)
automagik-spark workflows sync flow-abc-123

# 4. Create schedule (interactive)
automagik-spark schedules create
# Select workflow: 0
# Select type: 1 (Cron)
# Enter expression: */15 * * * *  (every 15 minutes)
# Enter input: {"param": "value"}

# 5. Verify schedule
automagik-spark schedules list

# 6. Test immediately
automagik-spark workflows run workflow-123 --input "test"

# 7. Check execution
automagik-spark tasks list
```

---

## Understanding Schedule Timing

<Info>
**Important**: Schedules use **UTC timezone** by default unless specified otherwise in the workflow source configuration.
</Info>

### Cron expression helper:

- `*/5 * * * *` = Every 5 minutes
- `0 * * * *` = Every hour (at minute 0)
- `0 0 * * *` = Every day at midnight
- `0 9 * * 1-5` = Weekdays at 9 AM
- `0 0 1 * *` = First day of every month

Use [crontab.guru](https://crontab.guru) to build complex expressions.

### Interval examples:

- `1m` = Every 1 minute
- `5m` = Every 5 minutes
- `30m` = Every 30 minutes
- `1h` = Every hour
- `6h` = Every 6 hours
- `1d` = Every day

---

## Managing Schedules

### Pause a schedule:

```bash
automagik-spark schedules update schedule-abc-123 pause
```

### Resume a schedule:

```bash
automagik-spark schedules update schedule-abc-123 resume
```

### Delete a schedule:

```bash
automagik-spark schedules delete schedule-abc-123
```

### Update schedule expression:

```bash
automagik-spark schedules set-expression schedule-abc-123 "0 */2 * * *"
```

### Update schedule input:

```bash
automagik-spark schedules set-input schedule-abc-123 '{"new": "data"}'
```

---

## Next Steps

<CardGroup cols={2}>
  <Card title="API Reference" icon="code" href="/spark/api/rest-overview">
    Explore the REST API endpoints
  </Card>
  <Card title="CLI Commands" icon="terminal" href="/spark/cli/overview">
    Complete CLI command reference
  </Card>
  <Card title="LangFlow Integration" icon="diagram-project" href="/spark/examples/langflow-integration">
    Deep dive into LangFlow workflows
  </Card>
  <Card title="Troubleshooting" icon="wrench" href="/spark/troubleshooting/common-errors">
    Common issues and solutions
  </Card>
</CardGroup>

---

## Quick Reference

### Essential Commands

```bash
# Sources
automagik-spark sources list
automagik-spark sources add --name NAME --type TYPE --url URL --api-key KEY

# Workflows
automagik-spark workflows sync
automagik-spark workflows sync FLOW_ID
automagik-spark workflows list
automagik-spark workflows run WORKFLOW_ID --input "data"

# Schedules
automagik-spark schedules create
automagik-spark schedules list
automagik-spark schedules update SCHEDULE_ID pause|resume|stop
automagik-spark schedules delete SCHEDULE_ID

# Tasks
automagik-spark tasks list
automagik-spark tasks view TASK_ID

# Workers
automagik-spark worker start
automagik-spark worker status
automagik-spark worker logs
```

---

**Congratulations!** You've scheduled your first workflow with Spark. Your AI workflows now run on autopilot.
