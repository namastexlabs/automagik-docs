---
title: "Installation"
description: "Get Spark up and running"
icon: "download"
---

## Prerequisites

Spark requires the following components to run:

<AccordionGroup>
  <Accordion title="Python 3.12 or higher (Required)">
    ```bash
    # Check your Python version
    python --version
    # Should output: Python 3.12.0 or higher
    ```

    Spark uses Python 3.12+ features. Install if needed:
    - **macOS**: `brew install python@3.12`
    - **Ubuntu/Debian**: `sudo apt install python3.12 python3-pip`
    - **Windows**: Download from [python.org](https://www.python.org)
  </Accordion>

  <Accordion title="PostgreSQL 12+ (Required)">
    ```bash
    # Check PostgreSQL version
    psql --version
    # Should output: psql (PostgreSQL) 12.0 or higher
    ```

    Spark stores all state in PostgreSQL:
    - **macOS**: `brew install postgresql@15`
    - **Ubuntu/Debian**: `sudo apt install postgresql-15`
    - **Docker**: Use the Docker Compose setup below
  </Accordion>

  <Accordion title="Redis 6+ (Required)">
    ```bash
    # Check Redis version
    redis-cli --version
    # Should output: redis-cli 6.0.0 or higher
    ```

    Redis powers the task queue:
    - **macOS**: `brew install redis`
    - **Ubuntu/Debian**: `sudo apt install redis-server`
    - **Docker**: Use the Docker Compose setup below
  </Accordion>
</AccordionGroup>

<Warning>
All three components (Python, PostgreSQL, Redis) are **required**. Spark will not function without them.
</Warning>

---

## Installation Methods

<Tabs>
  <Tab title="pip (For Development)">
    Install Spark via pip:

    ```bash
    # Install from PyPI
    pip install automagik-spark
    ```

    Verify installation:

    ```bash
    # Check version (command is automagik-spark, not spark)
    automagik-spark --help
    ```

    <Info>
    The CLI command is `automagik-spark`, not just `spark`.
    </Info>
  </Tab>

  <Tab title="Docker Compose (Recommended)">
    The easiest way to get everything running:

    ```bash
    # Clone the repository
    git clone https://github.com/namastexlabs/automagik-spark.git
    cd automagik-spark

    # Run setup script
    ./scripts/setup_local.sh
    ```

    This creates:
    - PostgreSQL (port 5402)
    - Redis (port 5412)
    - Spark API (port 8883)
    - Spark Workers (background)

    Verify it's running:
    ```bash
    curl http://localhost:8883/health
    # Should return: {"status":"ok"}
    ```
  </Tab>

  <Tab title="From Source">
    For development work:

    ```bash
    # Clone repository
    git clone https://github.com/namastexlabs/automagik-spark.git
    cd automagik-spark

    # Run development setup
    ./scripts/setup_dev.sh

    # Activate virtual environment
    source .venv/bin/activate

    # Verify installation
    automagik-spark --help
    ```
  </Tab>
</Tabs>

---

## Configuration

Spark uses environment variables for configuration. Create a `.env` file:

```bash
# Database (PostgreSQL)
DATABASE_URL=postgresql://spark_user:spark_pass@localhost:5402/automagik_spark

# Redis (Task Queue)
REDIS_URL=redis://:spark_redis_pass@localhost:5412/0
CELERY_BROKER_URL=redis://:spark_redis_pass@localhost:5412/0
CELERY_RESULT_BACKEND=redis://:spark_redis_pass@localhost:5412/1

# API Server
API_HOST=0.0.0.0
API_PORT=8883

# Encryption (generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
ENCRYPTION_KEY=your-generated-key-here

# Telemetry (optional)
AUTOMAGIK_SPARK_DISABLE_TELEMETRY=false
```

<Tip>
If using Docker Compose, these values are already configured in the provided `.env.example` file.
</Tip>

---

## Database Setup

Initialize the database:

```bash
# Activate virtual environment (if installed via pip)
source .venv/bin/activate

# Run database migrations
automagik-spark db upgrade
```

You should see output like:
```
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> abc123, Initial schema
```

---

## Start Spark Components

Spark consists of three components that need to run:

<Steps>
  <Step title="Start the API Server">
    ```bash
    automagik-spark api start
    ```

    The API will be available at `http://localhost:8883`
  </Step>

  <Step title="Start Workers (in another terminal)">
    ```bash
    automagik-spark worker start
    ```

    Workers execute scheduled tasks
  </Step>

  <Step title="Verify Everything is Running">
    ```bash
    # Check API health
    curl http://localhost:8883/health

    # Check worker status
    automagik-spark worker status
    ```
  </Step>
</Steps>

<Info>
**Docker Compose users**: All components start automatically with `docker-compose up -d`
</Info>

---

## Verify Installation

Test the complete setup:

```bash
# List sources (should be empty initially)
automagik-spark sources list

# Check API documentation
open http://localhost:8883/api/v1/docs

# View worker logs
automagik-spark worker logs
```

---

## Docker Compose Setup (Complete Example)

For production or easy setup, use this `docker-compose.yml`:

```yaml
services:
  # PostgreSQL database
  automagik-spark-db:
    image: postgres:15
    container_name: automagik-spark-db
    environment:
      POSTGRES_USER: spark_user
      POSTGRES_PASSWORD: spark_pass
      POSTGRES_DB: automagik_spark
    ports:
      - "5402:5402"
    command: postgres -p 5402
    volumes:
      - spark-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U spark_user -p 5402"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Redis for task queue
  redis:
    image: redis:7.4.2-alpine
    container_name: automagik-spark-redis
    ports:
      - "5412:5412"
    command: redis-server --port 5412 --appendonly yes --requirepass spark_redis_pass
    volumes:
      - spark-redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "5412", "-a", "spark_redis_pass", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Spark API server
  automagik-spark-api:
    image: namastexlabs/automagik-spark-api:latest
    container_name: automagik-spark-api
    ports:
      - "8883:8883"
    environment:
      DATABASE_URL: postgresql://spark_user:spark_pass@automagik-spark-db:5402/automagik_spark
      REDIS_URL: redis://:spark_redis_pass@redis:5412/0
      CELERY_BROKER_URL: redis://:spark_redis_pass@redis:5412/0
      CELERY_RESULT_BACKEND: redis://:spark_redis_pass@redis:5412/1
    depends_on:
      automagik-spark-db:
        condition: service_healthy
      redis:
        condition: service_healthy

  # Spark workers
  automagik-spark-worker:
    image: namastexlabs/automagik-spark-worker:latest
    container_name: automagik-spark-worker
    environment:
      DATABASE_URL: postgresql://spark_user:spark_pass@automagik-spark-db:5402/automagik_spark
      REDIS_URL: redis://:spark_redis_pass@redis:5412/0
      CELERY_BROKER_URL: redis://:spark_redis_pass@redis:5412/0
      CELERY_RESULT_BACKEND: redis://:spark_redis_pass@redis:5412/1
    depends_on:
      - automagik-spark-api

volumes:
  spark-db-data:
  spark-redis-data:
```

Start with:
```bash
docker-compose up -d
```

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="Connection refused to PostgreSQL">
    **Error**: `psycopg2.OperationalError: could not connect to server`

    **Fix**:
    ```bash
    # Check if PostgreSQL is running
    pg_isready -h localhost -p 5402

    # Check DATABASE_URL in .env matches your PostgreSQL setup
    echo $DATABASE_URL

    # Verify PostgreSQL is listening on the correct port
    sudo netstat -tulpn | grep 5402
    ```
  </Accordion>

  <Accordion title="Connection refused to Redis">
    **Error**: `redis.exceptions.ConnectionError: Error 111 connecting to localhost:5412`

    **Fix**:
    ```bash
    # Check if Redis is running
    redis-cli -p 5412 -a spark_redis_pass ping
    # Should return: PONG

    # Check REDIS_URL in .env
    echo $REDIS_URL

    # Start Redis if not running
    redis-server --port 5412
    ```
  </Accordion>

  <Accordion title="Command 'automagik-spark' not found">
    **Error**: `bash: automagik-spark: command not found`

    **Fix**:
    ```bash
    # Make sure you installed the package
    pip install automagik-spark

    # Or activate virtual environment
    source .venv/bin/activate

    # Verify installation
    python -m automagik_spark --help
    ```
  </Accordion>

  <Accordion title="Database migration fails">
    **Error**: `alembic.util.exc.CommandError: Can't locate revision`

    **Fix**:
    ```bash
    # Reset database (WARNING: deletes all data)
    dropdb -h localhost -p 5402 -U spark_user automagik_spark
    createdb -h localhost -p 5402 -U spark_user automagik_spark

    # Run migrations again
    automagik-spark db upgrade
    ```
  </Accordion>
</AccordionGroup>

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Quick Start" icon="rocket" href="/spark/quickstart">
    Schedule your first workflow in 30 minutes
  </Card>
  <Card title="Introduction" icon="book" href="/spark/introduction">
    Understand how Spark works
  </Card>
</CardGroup>
